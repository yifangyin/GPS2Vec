# GPS2Vec+: Learning Multi-context Aware Location Representations from Large-scale Geotagged Images

Pretrained models for our papers:

**<a href="/papers/gps2vec+.pdf">Learning Multi-context Aware Location Representations from Large-scale Geotagged Images</a>** 

**<a href="/papers/gps2vec.pdf">GPS2Vec: Towards Generating Worldwide GPS Embeddings</a>** 

## Requirements
  - numpy
  - math
  - utm
  - keras

## Usage

### Download Pre-trained Models

**<a href="https://www.dropbox.com/s/j8b4h3ynkv42gj4/models_tag.zip?dl=0">models_tag</a>** Semantic context trained on the 1M Flickr dataset.

**<a href="https://www.dropbox.com/s/kcsadz2fl6ynymh/models_visual.zip?dl=0">models_visual</a>** Visual context trained on the YLI-GEO dataset.

## Citation
If you use the pre-trained models please cite our paper.
```
@inproceedings{yin2021-gps2vec+,
  title={Learning Multi-context Aware Location Representations from Large-scale Geotagged Images},
  author={Yin, Yifang and Zhang, Ying and Liu, Zhenguang and Liang, Yuxuan and Wang, Sheng and Shah, Rajiv Ratn and Zimmermann, Roger},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  year={2021}
}

@inproceedings{yin2019-gps2vec,
author = {Yin, Yifang and Liu, Zhenguang and Zhang, Ying and Wang, Sheng and Shah, Rajiv Ratn and Zimmermann, Roger},
title = {GPS2Vec: Towards Generating Worldwide GPS Embeddings},
year = {2019},
booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {416â€“-419},
}
```
